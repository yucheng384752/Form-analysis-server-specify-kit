"""
æ¸¬è©¦ Pydantic Schemas
"""
import json
from datetime import datetime
from pydantic import ValidationError

# ç›´æ¥å®šç¾© schemas ç”¨æ–¼æ¸¬è©¦
from pydantic import BaseModel, Field, field_validator
from typing import Optional
from enum import Enum

class JobStatus(str, Enum):
    """å·¥ä½œç‹€æ…‹"""
    PENDING = "PENDING"
    PROCESSING = "PROCESSING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"

# Upload Job Schemas
class UploadJobCreate(BaseModel):
    """å‰µå»ºä¸Šå‚³å·¥ä½œçš„è«‹æ±‚æ¨¡å‹"""
    filename: str = Field(..., min_length=1, max_length=255, description="ä¸Šå‚³æª”æ¡ˆåç¨±")
    file_size: int = Field(..., ge=1, description="æª”æ¡ˆå¤§å°ï¼ˆä½å…ƒçµ„ï¼‰")
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "filename": "products_202401.xlsx",
                "file_size": 2048000
            }
        }
    }

class UploadJobRead(BaseModel):
    """ä¸Šå‚³å·¥ä½œå›æ‡‰æ¨¡å‹"""
    id: str = Field(..., description="å·¥ä½œå”¯ä¸€è­˜åˆ¥ç¢¼")
    filename: str = Field(..., description="ä¸Šå‚³æª”æ¡ˆåç¨±")
    file_size: int = Field(..., description="æª”æ¡ˆå¤§å°ï¼ˆä½å…ƒçµ„ï¼‰")
    status: JobStatus = Field(..., description="è™•ç†ç‹€æ…‹")
    created_at: datetime = Field(..., description="å»ºç«‹æ™‚é–“")
    processed_at: Optional[datetime] = Field(None, description="è™•ç†å®Œæˆæ™‚é–“")
    total_records: Optional[int] = Field(None, description="ç¸½è¨˜éŒ„æ•¸")
    success_records: Optional[int] = Field(None, description="æˆåŠŸè¨˜éŒ„æ•¸")
    error_records: Optional[int] = Field(None, description="éŒ¯èª¤è¨˜éŒ„æ•¸")
    
    model_config = {
        "from_attributes": True,
        "json_schema_extra": {
            "example": {
                "id": "550e8400-e29b-41d4-a716-446655440000",
                "filename": "products_202401.xlsx",
                "file_size": 2048000,
                "status": "COMPLETED",
                "created_at": "2024-01-08T10:30:00Z",
                "processed_at": "2024-01-08T10:35:00Z",
                "total_records": 150,
                "success_records": 148,
                "error_records": 2
            }
        }
    }

# Record Schemas
class RecordCreate(BaseModel):
    """å‰µå»ºè¨˜éŒ„çš„è«‹æ±‚æ¨¡å‹"""
    lot_no: str = Field(..., min_length=1, max_length=50, description="æ‰¹è™Ÿ")
    product_name: Optional[str] = Field(None, max_length=200, description="ç”¢å“åç¨±")
    specification: Optional[str] = Field(None, description="è¦æ ¼èªªæ˜")
    quantity: Optional[int] = Field(None, ge=0, description="æ•¸é‡")
    unit: Optional[str] = Field(None, max_length=20, description="å–®ä½")
    raw_data: dict = Field(..., description="åŸå§‹è³‡æ–™")
    
    @field_validator("lot_no")
    @classmethod
    def validate_lot_no_format(cls, v: str) -> str:
        """é©—è­‰æ‰¹è™Ÿæ ¼å¼ (L + æ—¥æœŸ + åºè™Ÿ)"""
        import re
        # å…ˆè½‰ç‚ºå¤§å¯«
        v = v.upper()
        if not re.match(r"^L\d{6}\d{3}$", v):
            raise ValueError("æ‰¹è™Ÿæ ¼å¼å¿…é ˆç‚º L + 6ä½æ—¥æœŸ + 3ä½åºè™Ÿ (ä¾‹å¦‚: L240108001)")
        return v
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "lot_no": "L240108001",
                "product_name": "ç”¢å“Aå‹è™Ÿ",
                "specification": "è¦æ ¼èªªæ˜æ–‡å­—",
                "quantity": 100,
                "unit": "pcs",
                "raw_data": {
                    "column_a": "å€¼A",
                    "column_b": "å€¼B"
                }
            }
        }
    }

class RecordRead(BaseModel):
    """è¨˜éŒ„å›æ‡‰æ¨¡å‹"""
    id: str = Field(..., description="è¨˜éŒ„å”¯ä¸€è­˜åˆ¥ç¢¼")
    upload_job_id: str = Field(..., description="æ‰€å±¬ä¸Šå‚³å·¥ä½œID")
    lot_no: str = Field(..., description="æ‰¹è™Ÿ")
    product_name: Optional[str] = Field(None, description="ç”¢å“åç¨±")
    specification: Optional[str] = Field(None, description="è¦æ ¼èªªæ˜")
    quantity: Optional[int] = Field(None, description="æ•¸é‡")
    unit: Optional[str] = Field(None, description="å–®ä½")
    raw_data: dict = Field(..., description="åŸå§‹è³‡æ–™")
    created_at: datetime = Field(..., description="å»ºç«‹æ™‚é–“")
    
    model_config = {
        "from_attributes": True,
        "json_schema_extra": {
            "example": {
                "id": "550e8400-e29b-41d4-a716-446655440001",
                "upload_job_id": "550e8400-e29b-41d4-a716-446655440000",
                "lot_no": "L240108001",
                "product_name": "ç”¢å“Aå‹è™Ÿ",
                "specification": "è¦æ ¼èªªæ˜æ–‡å­—",
                "quantity": 100,
                "unit": "pcs",
                "raw_data": {
                    "column_a": "å€¼A",
                    "column_b": "å€¼B"
                },
                "created_at": "2024-01-08T10:30:00Z"
            }
        }
    }

# Upload Error Schemas
class UploadErrorCreate(BaseModel):
    """å‰µå»ºä¸Šå‚³éŒ¯èª¤çš„è«‹æ±‚æ¨¡å‹"""
    error_message: str = Field(..., min_length=1, description="éŒ¯èª¤è¨Šæ¯")
    error_details: Optional[dict] = Field(None, description="éŒ¯èª¤è©³ç´°è³‡è¨Š")
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "error_message": "è³‡æ–™æ ¼å¼éŒ¯èª¤",
                "error_details": {
                    "row": 5,
                    "column": "B",
                    "expected": "æ•¸å­—",
                    "received": "æ–‡å­—"
                }
            }
        }
    }

class UploadErrorRead(BaseModel):
    """ä¸Šå‚³éŒ¯èª¤å›æ‡‰æ¨¡å‹"""
    id: str = Field(..., description="éŒ¯èª¤å”¯ä¸€è­˜åˆ¥ç¢¼")
    upload_job_id: str = Field(..., description="æ‰€å±¬ä¸Šå‚³å·¥ä½œID")
    error_message: str = Field(..., description="éŒ¯èª¤è¨Šæ¯")
    error_details: Optional[dict] = Field(None, description="éŒ¯èª¤è©³ç´°è³‡è¨Š")
    created_at: datetime = Field(..., description="å»ºç«‹æ™‚é–“")
    
    model_config = {
        "from_attributes": True,
        "json_schema_extra": {
            "example": {
                "id": "550e8400-e29b-41d4-a716-446655440002",
                "upload_job_id": "550e8400-e29b-41d4-a716-446655440000",
                "error_message": "è³‡æ–™æ ¼å¼éŒ¯èª¤",
                "error_details": {
                    "row": 5,
                    "column": "B",
                    "expected": "æ•¸å­—",
                    "received": "æ–‡å­—"
                },
                "created_at": "2024-01-08T10:32:00Z"
            }
        }
    }

def test_schemas():
    """æ¸¬è©¦æ‰€æœ‰ Pydantic schemas"""
    print("ğŸš€ é–‹å§‹æ¸¬è©¦ Pydantic Schemas\n")
    
    try:
        # 1. æ¸¬è©¦ UploadJobCreate
        print("ğŸ“ æ¸¬è©¦ UploadJobCreate...")
        job_create_data = {
            "filename": "test_file.xlsx",
            "file_size": 1024000
        }
        job_create = UploadJobCreate(**job_create_data)
        print(f" å‰µå»ºæˆåŠŸ: {job_create.filename}, {job_create.file_size} bytes")
        
        # æ¸¬è©¦é©—è­‰éŒ¯èª¤
        try:
            invalid_job = UploadJobCreate(filename="", file_size=0)
        except ValidationError as e:
            print(f" é©—è­‰éŒ¯èª¤æ­£ç¢ºæ•ç²: {len(e.errors())} å€‹éŒ¯èª¤")
        
        # 2. æ¸¬è©¦ RecordCreate
        print(f"\nğŸ“ æ¸¬è©¦ RecordCreate...")
        record_create_data = {
            "lot_no": "L240108001",
            "product_name": "æ¸¬è©¦ç”¢å“",
            "specification": "æ¸¬è©¦è¦æ ¼",
            "quantity": 50,
            "unit": "pcs",
            "raw_data": {"col1": "value1", "col2": "value2"}
        }
        record_create = RecordCreate(**record_create_data)
        print(f" å‰µå»ºæˆåŠŸ: {record_create.lot_no}, {record_create.product_name}")
        
        # æ¸¬è©¦æ‰¹è™Ÿæ ¼å¼é©—è­‰
        try:
            invalid_record = RecordCreate(
                lot_no="INVALID",
                raw_data={"test": "data"}
            )
        except ValidationError as e:
            print(f" æ‰¹è™Ÿæ ¼å¼é©—è­‰éŒ¯èª¤æ­£ç¢ºæ•ç²: {e.errors()[0]['msg']}")
        
        # æ¸¬è©¦æ­£ç¢ºçš„æ‰¹è™Ÿæ ¼å¼
        valid_record = RecordCreate(
            lot_no="l240108999",  # å°å¯«ï¼Œæ‡‰è©²è½‰ç‚ºå¤§å¯«
            raw_data={"test": "data"}
        )
        print(f" æ‰¹è™Ÿå¤§å¯«è½‰æ›: {valid_record.lot_no}")
        
        # 3. æ¸¬è©¦ UploadErrorCreate
        print(f"\nğŸ“ æ¸¬è©¦ UploadErrorCreate...")
        error_create_data = {
            "error_message": "æ¸¬è©¦éŒ¯èª¤è¨Šæ¯",
            "error_details": {
                "row": 10,
                "column": "C",
                "issue": "è³‡æ–™é¡å‹éŒ¯èª¤"
            }
        }
        error_create = UploadErrorCreate(**error_create_data)
        print(f" å‰µå»ºæˆåŠŸ: {error_create.error_message}")
        
        # 4. æ¸¬è©¦ Read schemas çš„ JSON åºåˆ—åŒ–
        print(f"\nğŸ“ æ¸¬è©¦ Read schemas JSON åºåˆ—åŒ–...")
        
        # UploadJobRead
        job_read_data = {
            "id": "550e8400-e29b-41d4-a716-446655440000",
            "filename": "test_file.xlsx",
            "file_size": 1024000,
            "status": JobStatus.COMPLETED,
            "created_at": datetime.now(),
            "processed_at": datetime.now(),
            "total_records": 100,
            "success_records": 98,
            "error_records": 2
        }
        job_read = UploadJobRead(**job_read_data)
        job_json = job_read.model_dump_json()
        print(f" UploadJobRead JSON åºåˆ—åŒ–æˆåŠŸ ({len(job_json)} chars)")
        
        # RecordRead  
        record_read_data = {
            "id": "550e8400-e29b-41d4-a716-446655440001",
            "upload_job_id": "550e8400-e29b-41d4-a716-446655440000",
            "lot_no": "L240108001",
            "product_name": "æ¸¬è©¦ç”¢å“",
            "specification": "æ¸¬è©¦è¦æ ¼",
            "quantity": 50,
            "unit": "pcs",
            "raw_data": {"col1": "value1", "col2": "value2"},
            "created_at": datetime.now()
        }
        record_read = RecordRead(**record_read_data)
        record_json = record_read.model_dump_json()
        print(f" RecordRead JSON åºåˆ—åŒ–æˆåŠŸ ({len(record_json)} chars)")
        
        # UploadErrorRead
        error_read_data = {
            "id": "550e8400-e29b-41d4-a716-446655440002",
            "upload_job_id": "550e8400-e29b-41d4-a716-446655440000",
            "error_message": "æ¸¬è©¦éŒ¯èª¤è¨Šæ¯",
            "error_details": {"row": 10, "column": "C", "issue": "è³‡æ–™é¡å‹éŒ¯èª¤"},
            "created_at": datetime.now()
        }
        error_read = UploadErrorRead(**error_read_data)
        error_json = error_read.model_dump_json()
        print(f" UploadErrorRead JSON åºåˆ—åŒ–æˆåŠŸ ({len(error_json)} chars)")
        
        # 5. æ¸¬è©¦ JSON Schema ç”Ÿæˆ
        print(f"\nğŸ“ æ¸¬è©¦ JSON Schema ç”Ÿæˆ...")
        
        job_schema = UploadJobCreate.model_json_schema()
        print(f" UploadJobCreate schema: {len(job_schema['properties'])} å€‹å±¬æ€§")
        
        record_schema = RecordCreate.model_json_schema()
        print(f" RecordCreate schema: {len(record_schema['properties'])} å€‹å±¬æ€§")
        
        error_schema = UploadErrorCreate.model_json_schema()
        print(f" UploadErrorCreate schema: {len(error_schema['properties'])} å€‹å±¬æ€§")
        
        print(f"\nğŸ‰ æ‰€æœ‰ Pydantic Schema æ¸¬è©¦é€šé!")
        print(f"\nğŸ“‹ æ¸¬è©¦è¦†è“‹:")
        print(f"    Create schemas é©—è­‰")
        print(f"    æ¬„ä½é©—è­‰å™¨")
        print(f"    éŒ¯èª¤è™•ç†")
        print(f"    JSON åºåˆ—åŒ–")
        print(f"    Schema ç”Ÿæˆ")
        print(f"    ç¯„ä¾‹è³‡æ–™")
        
        return True
        
    except Exception as e:
        print(f" æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_schemas()