"""
æ‰‹å‹•æ¸¬è©¦ SQLAlchemy æ¨¡å‹ - ä½¿ç”¨ç›´æ¥å®šç¾©
"""
import asyncio
import uuid
from datetime import datetime
from pathlib import Path
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy import String, Integer, DateTime, Text, JSON, ForeignKey, Index, select
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship
from enum import Enum

# SQLite URL
DATABASE_URL = "sqlite+aiosqlite:///./manual_test.db"

# Base é¡
class Base(DeclarativeBase):
    """Base class for all database models."""
    pass

# Job Status Enum
class JobStatus(str, Enum):
    """ä¸Šå‚³å·¥ä½œç‹€æ…‹åˆ—èˆ‰"""
    PENDING = "PENDING"
    PROCESSING = "PROCESSING" 
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"

# æ‰‹å‹•å®šç¾©æ¨¡å‹ (é©ç”¨æ–¼ SQLite)
class UploadJob(Base):
    """ä¸Šå‚³å·¥ä½œæ¨¡å‹"""
    __tablename__ = "upload_jobs"
    
    id: Mapped[str] = mapped_column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    filename: Mapped[str] = mapped_column(String(255), nullable=False)
    file_size: Mapped[int] = mapped_column(Integer, nullable=False)
    status: Mapped[JobStatus] = mapped_column(String(20), nullable=False, default=JobStatus.PENDING)
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False, default=datetime.utcnow)
    processed_at: Mapped[datetime | None] = mapped_column(DateTime, nullable=True)
    total_records: Mapped[int | None] = mapped_column(Integer, nullable=True)
    success_records: Mapped[int | None] = mapped_column(Integer, nullable=True) 
    error_records: Mapped[int | None] = mapped_column(Integer, nullable=True)
    
    # é—œè¯
    records: Mapped[list["Record"]] = relationship("Record", back_populates="upload_job", cascade="all, delete-orphan")
    errors: Mapped[list["UploadError"]] = relationship("UploadError", back_populates="upload_job", cascade="all, delete-orphan")

class Record(Base):
    """è¨˜éŒ„æ¨¡å‹"""
    __tablename__ = "records"
    
    id: Mapped[str] = mapped_column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    upload_job_id: Mapped[str] = mapped_column(String(36), ForeignKey("upload_jobs.id", ondelete="CASCADE"), nullable=False)
    lot_no: Mapped[str] = mapped_column(String(50), nullable=False)
    product_name: Mapped[str | None] = mapped_column(String(200), nullable=True)
    specification: Mapped[str | None] = mapped_column(Text, nullable=True)
    quantity: Mapped[int | None] = mapped_column(Integer, nullable=True)
    unit: Mapped[str | None] = mapped_column(String(20), nullable=True)
    raw_data: Mapped[dict] = mapped_column(JSON, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False, default=datetime.utcnow)
    
    # é—œè¯
    upload_job: Mapped["UploadJob"] = relationship("UploadJob", back_populates="records")

class UploadError(Base):
    """ä¸Šå‚³éŒ¯èª¤æ¨¡å‹"""
    __tablename__ = "upload_errors"
    
    id: Mapped[str] = mapped_column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    upload_job_id: Mapped[str] = mapped_column(String(36), ForeignKey("upload_jobs.id", ondelete="CASCADE"), nullable=False)
    error_message: Mapped[str] = mapped_column(Text, nullable=False)
    error_details: Mapped[dict | None] = mapped_column(JSON, nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False, default=datetime.utcnow)
    
    # é—œè¯
    upload_job: Mapped["UploadJob"] = relationship("UploadJob", back_populates="errors")

async def test_manual_models():
    """æ‰‹å‹•æ¸¬è©¦æ¨¡å‹"""
    print("ğŸš€ é–‹å§‹æ‰‹å‹•æ¸¬è©¦ SQLAlchemy æ¨¡å‹\n")
    
    try:
        # å‰µå»ºå¼•æ“
        engine = create_async_engine(DATABASE_URL, echo=True)
        
        # å‰µå»º session factory
        session_factory = async_sessionmaker(
            engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
        
        print(" å‰µå»ºè³‡æ–™åº«è¡¨æ ¼...")
        
        # å‰µå»ºæ‰€æœ‰è¡¨æ ¼
        async with engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        
        print(" è¡¨æ ¼å‰µå»ºæˆåŠŸ!\n")
        
        # æ¸¬è©¦ CRUD æ“ä½œ
        async with session_factory() as session:
            print("ğŸ“ æ¸¬è©¦å‰µå»ºè¨˜éŒ„...")
            
            # 1. å‰µå»º UploadJob
            job = UploadJob(
                filename="manual_test_upload.xlsx",
                file_size=3072000,
                status=JobStatus.PENDING
            )
            session.add(job)
            await session.commit()
            await session.refresh(job)
            
            print(f" å‰µå»ºå·¥ä½œ: {job.id} - {job.filename}")
            
            # 2. å‰µå»º Record
            record1 = Record(
                upload_job_id=job.id,
                lot_no="L240108001",
                product_name="æ‰‹å‹•æ¸¬è©¦ç”¢å“ A",
                specification="æ¸¬è©¦è¦æ ¼èªªæ˜",
                quantity=200,
                unit="pcs",
                raw_data={"test": "manual", "type": "record1"}
            )
            
            record2 = Record(
                upload_job_id=job.id,
                lot_no="L240108002", 
                product_name="æ‰‹å‹•æ¸¬è©¦ç”¢å“ B",
                specification="å¦ä¸€å€‹æ¸¬è©¦è¦æ ¼",
                quantity=150,
                unit="kg",
                raw_data={"test": "manual", "type": "record2"}
            )
            
            session.add_all([record1, record2])
            await session.commit()
            await session.refresh(record1)
            await session.refresh(record2)
            
            print(f" å‰µå»ºè¨˜éŒ„1: {record1.id} - {record1.lot_no}")
            print(f" å‰µå»ºè¨˜éŒ„2: {record2.id} - {record2.lot_no}")
            
            # 3. å‰µå»º UploadError
            error = UploadError(
                upload_job_id=job.id,
                error_message="æ‰‹å‹•æ¸¬è©¦éŒ¯èª¤è¨Šæ¯",
                error_details={"row": 10, "column": "C", "issue": "æ•¸å€¼æ ¼å¼éŒ¯èª¤"}
            )
            session.add(error)
            await session.commit()
            await session.refresh(error)
            
            print(f" å‰µå»ºéŒ¯èª¤: {error.id}")
            
            # 4. æ¸¬è©¦é—œè¯æŸ¥è©¢
            print(f"\n æ¸¬è©¦é—œè¯æŸ¥è©¢...")
            
            # æŸ¥è©¢å·¥ä½œåŠå…¶è¨˜éŒ„
            result = await session.execute(
                select(UploadJob).where(UploadJob.id == job.id)
            )
            job_with_data = result.scalar_one()
            
            # è¼‰å…¥é—œè¯ - æ‰‹å‹•æŸ¥è©¢
            records_result = await session.execute(
                select(Record).where(Record.upload_job_id == job.id)
            )
            job_records = records_result.scalars().all()
            
            errors_result = await session.execute(
                select(UploadError).where(UploadError.upload_job_id == job.id)
            )
            job_errors = errors_result.scalars().all()
            
            print(f"   å·¥ä½œ {job_with_data.id}:")
            print(f"     - æª”å: {job_with_data.filename}")
            print(f"     - ç‹€æ…‹: {job_with_data.status}")
            print(f"     - è¨˜éŒ„æ•¸é‡: {len(job_records)}")
            print(f"     - éŒ¯èª¤æ•¸é‡: {len(job_errors)}")
            
            # 5. æ¸¬è©¦æ›´æ–°
            print(f"\nğŸ“ æ¸¬è©¦æ›´æ–°æ“ä½œ...")
            job_with_data.status = JobStatus.COMPLETED
            job_with_data.processed_at = datetime.utcnow()
            job_with_data.total_records = len(job_records)
            job_with_data.success_records = len(job_records)
            job_with_data.error_records = len(job_errors)
            
            await session.commit()
            
            print(f" æ›´æ–°å·¥ä½œç‹€æ…‹ç‚º: {job_with_data.status}")
            print(f" è¨­ç½®çµ±è¨ˆ: ç¸½è¨ˆ{job_with_data.total_records}, æˆåŠŸ{job_with_data.success_records}, éŒ¯èª¤{job_with_data.error_records}")
            
            # 6. æ¸¬è©¦æŸ¥è©¢çµ±è¨ˆ
            print(f"\n æ¸¬è©¦æŸ¥è©¢çµ±è¨ˆ...")
            
            all_jobs = await session.execute(select(UploadJob))
            job_count = len(all_jobs.scalars().all())
            
            all_records = await session.execute(select(Record))
            record_count = len(all_records.scalars().all())
            
            all_errors = await session.execute(select(UploadError))
            error_count = len(all_errors.scalars().all())
            
            print(f"   ç¸½å·¥ä½œæ•¸: {job_count}")
            print(f"   ç¸½è¨˜éŒ„æ•¸: {record_count}")  
            print(f"   ç¸½éŒ¯èª¤æ•¸: {error_count}")
            
            # 7. æ¸¬è©¦è¤‡é›œæŸ¥è©¢
            print(f"\n æ¸¬è©¦è¤‡é›œæŸ¥è©¢...")
            
            # æŸ¥è©¢ç‰¹å®šæ‰¹è™Ÿ
            lot_result = await session.execute(
                select(Record).where(Record.lot_no.like("L240108%"))
            )
            lot_records = lot_result.scalars().all()
            print(f"   æ‰¹è™ŸåŒ…å« 'L240108' çš„è¨˜éŒ„: {len(lot_records)}")
            
            # æŸ¥è©¢å·²å®Œæˆçš„å·¥ä½œ
            completed_result = await session.execute(
                select(UploadJob).where(UploadJob.status == JobStatus.COMPLETED)
            )
            completed_jobs = completed_result.scalars().all()
            print(f"   å·²å®Œæˆçš„å·¥ä½œ: {len(completed_jobs)}")
        
        await engine.dispose()
        
        print(f"\nğŸ‰ æ‰‹å‹•æ¨¡å‹æ¸¬è©¦å…¨éƒ¨é€šé!")
        print(f"   è³‡æ–™åº«æª”æ¡ˆ: {Path('manual_test.db').absolute()}")
        print(f"\nğŸ“‹ æ¸¬è©¦è¦†è“‹:")
        print(f"    è¡¨æ ¼å‰µå»º")
        print(f"    CRUD æ“ä½œ (å‰µå»ºã€è®€å–ã€æ›´æ–°)")
        print(f"    é—œè¯æŸ¥è©¢")
        print(f"    è¤‡é›œæŸ¥è©¢")
        print(f"    çµ±è¨ˆåŠŸèƒ½")
        
        return True
        
    except Exception as e:
        print(f" æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    asyncio.run(test_manual_models())