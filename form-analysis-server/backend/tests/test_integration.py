"""
æ•´åˆæ¸¬è©¦ - æ¸¬è©¦ä¸‰å€‹æ¨¡å‹ä¹‹é–“çš„å®Œæ•´äº¤äº’
"""
import pytest
import uuid
from datetime import datetime, date
from sqlalchemy import select, func

from app.models.upload_job import UploadJob, JobStatus
from app.models.record import Record
from app.models.upload_error import UploadError
from tests.conftest import TestDataFactory


class TestModelsIntegration:
    """æ¨¡å‹æ•´åˆæ¸¬è©¦é¡"""
    
    @pytest.mark.asyncio
    async def test_complete_upload_workflow(self, db_session, clean_db):
        """æ¸¬è©¦å®Œæ•´çš„ä¸Šå‚³å·¥ä½œæµç¨‹"""
        
        # 1. å‰µå»ºä¸Šå‚³å·¥ä½œ
        job_data = TestDataFactory.upload_job_data(
            filename="integration_test.csv"
        )
        job = UploadJob(**job_data)
        db_session.add(job)
        await db_session.commit()
        await db_session.refresh(job)
        
        assert job.status == JobStatus.PENDING
        print(f"âœ… æ­¥é©Ÿ1: å‰µå»ºä¸Šå‚³å·¥ä½œ {job.id}")
        
        # 2. é–‹å§‹é©—è­‰
        job.status = JobStatus.VALIDATED
        await db_session.commit()
        print(f"âœ… æ­¥é©Ÿ2: é©—è­‰å®Œæˆï¼Œç‹€æ…‹æ›´æ–°ç‚º {job.status}")
        
        # 3. å‰µå»ºæˆåŠŸè¨˜éŒ„ï¼ˆé€™äº›è¨˜éŒ„ä¸èˆ‡ UploadJob ç›´æ¥é—œè¯ï¼‰
        success_records = []
        for i in range(5):
            record_data = TestDataFactory.record_data(
                lot_no=f"123456{i+1}_01",
                product_name=f"æˆåŠŸç”¢å“ {i+1}",
                quantity=100 + i * 10,
                production_date=date(2024, 1, i+1)
            )
            success_records.append(Record(**record_data))
        
        db_session.add_all(success_records)
        await db_session.commit()
        print(f"âœ… æ­¥é©Ÿ3: å‰µå»º {len(success_records)} ç­†æˆåŠŸè¨˜éŒ„")
        
        # 4. å‰µå»ºéŒ¯èª¤è¨˜éŒ„
        error_scenarios = [
            (7, "lot_no", "INVALID_FORMAT", "æ‰¹è™Ÿæ ¼å¼éŒ¯èª¤"),
            (9, "quantity", "NEGATIVE_VALUE", "æ•¸é‡å¿…é ˆç‚ºæ­£æ•¸")
        ]
        
        errors = []
        for row_index, field, error_code, message in error_scenarios:
            error_data = TestDataFactory.upload_error_data(
                job_id=job.id,
                row_index=row_index,
                field=field,
                error_code=error_code,
                message=message
            )
            errors.append(UploadError(**error_data))
        
        db_session.add_all(errors)
        await db_session.commit()
        print(f"âœ… æ­¥é©Ÿ4: å‰µå»º {len(errors)} ç­†éŒ¯èª¤è¨˜éŒ„")
        
        # 5. å®Œæˆè™•ç†ä¸¦æ›´æ–°çµ±è¨ˆ
        job.status = JobStatus.IMPORTED
        job.total_rows = len(success_records) + len(errors)
        job.valid_rows = len(success_records)
        job.invalid_rows = len(errors)
        await db_session.commit()
        print(f"âœ… æ­¥é©Ÿ5: è™•ç†å®Œæˆï¼Œçµ±è¨ˆä¿¡æ¯å·²æ›´æ–°")
        
        # 6. é©—è­‰å®Œæ•´æ€§
        
        # é©—è­‰å·¥ä½œç‹€æ…‹
        final_job_result = await db_session.execute(
            select(UploadJob).where(UploadJob.id == job.id)
        )
        final_job = final_job_result.scalar_one()
        
        assert final_job.status == JobStatus.IMPORTED
        assert final_job.total_rows == 7
        assert final_job.valid_rows == 5
        assert final_job.invalid_rows == 2
        
        # é©—è­‰æˆåŠŸè¨˜éŒ„
        success_result = await db_session.execute(select(Record))
        success_records_from_db = success_result.scalars().all()
        assert len(success_records_from_db) == 5
        
        # é©—è­‰éŒ¯èª¤è¨˜éŒ„
        error_result = await db_session.execute(
            select(UploadError).where(UploadError.job_id == job.id)
        )
        errors_from_db = error_result.scalars().all()
        assert len(errors_from_db) == 2
        
        # é©—è­‰å¤–éµé—œè¯ï¼ˆåªæœ‰ UploadError èˆ‡ UploadJob æœ‰é—œè¯ï¼‰
        assert all(error.job_id == job.id for error in errors_from_db)
        
        print(f"âœ… æ­¥é©Ÿ6: å®Œæ•´æ€§é©—è­‰é€šé")
        print(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ: ç¸½è¨ˆ{final_job.total_rows}ç­†ï¼Œæœ‰æ•ˆ{final_job.valid_rows}ç­†ï¼Œç„¡æ•ˆ{final_job.invalid_rows}ç­†")
    
    @pytest.mark.asyncio
    async def test_cascade_delete_integration(self, db_session, clean_db):
        """æ¸¬è©¦ç´šè¯åˆªé™¤çš„å®Œæ•´æµç¨‹"""
        
        # å‰µå»ºå·¥ä½œ
        job_data = TestDataFactory.upload_job_data(filename="cascade_test.csv")
        job = UploadJob(**job_data)
        db_session.add(job)
        await db_session.commit()
        await db_session.refresh(job)
        
        # å‰µå»ºè¨˜éŒ„ï¼ˆç¨ç«‹å­˜åœ¨ï¼Œä¸èˆ‡ UploadJob é—œè¯ï¼‰
        records = []
        for i in range(3):
            record_data = TestDataFactory.record_data(
                lot_no=f"CASCADE{i+1:02d}_01",
                product_name=f"ç´šè¯æ¸¬è©¦ç”¢å“ {i+1}",
                quantity=100 + i * 10,
                production_date=date(2024, 1, i+1)
            )
            records.append(Record(**record_data))
        
        # å‰µå»ºéŒ¯èª¤ï¼ˆèˆ‡ UploadJob é—œè¯ï¼‰
        errors = []
        for i in range(2):
            error_data = TestDataFactory.upload_error_data(
                job_id=job.id,
                row_index=i+1,
                field="test_field",
                error_code="TEST_ERROR",
                message=f"ç´šè¯æ¸¬è©¦éŒ¯èª¤ {i+1}"
            )
            errors.append(UploadError(**error_data))
        
        db_session.add_all(records + errors)
        await db_session.commit()
        
        # é©—è­‰è¨˜éŒ„å­˜åœ¨
        record_count = await db_session.scalar(select(func.count(Record.id)))
        error_count = await db_session.scalar(
            select(func.count(UploadError.id)).where(UploadError.job_id == job.id)
        )
        
        assert record_count == 3
        assert error_count == 2
        print(f"âœ… å‰µå»ºå®Œæˆ: {record_count} ç­†è¨˜éŒ„, {error_count} ç­†éŒ¯èª¤")
        
        # åˆªé™¤å·¥ä½œï¼ˆè§¸ç™¼ç´šè¯åˆªé™¤ï¼‰
        await db_session.delete(job)
        await db_session.commit()
        
        # é©—è­‰ Record ä¸å—å½±éŸ¿ï¼ˆæ²’æœ‰å¤–éµé—œè¯ï¼‰
        remaining_records = await db_session.scalar(select(func.count(Record.id)))
        
        # é©—è­‰ UploadError è¢«ç´šè¯åˆªé™¤
        remaining_errors = await db_session.scalar(
            select(func.count(UploadError.id)).where(UploadError.job_id == job.id)
        )
        
        assert remaining_records == 3  # Record ä¸å—å½±éŸ¿
        assert remaining_errors == 0   # UploadError è¢«ç´šè¯åˆªé™¤
        print(f"âœ… ç´šè¯åˆªé™¤æˆåŠŸ: éŒ¯èª¤å·²æ¸…é™¤ï¼Œè¨˜éŒ„ä¿æŒç¨ç«‹")
    
    @pytest.mark.asyncio
    async def test_multiple_jobs_isolation(self, db_session, clean_db):
        """æ¸¬è©¦å¤šå€‹å·¥ä½œä¹‹é–“çš„è³‡æ–™éš”é›¢"""
        
        # å‰µå»ºä¸‰å€‹ä¸åŒçš„å·¥ä½œ
        jobs = []
        for i in range(3):
            job_data = TestDataFactory.upload_job_data(
                filename=f"isolation_test_{i+1}.csv"
            )
            job = UploadJob(**job_data)
            jobs.append(job)
        
        db_session.add_all(jobs)
        await db_session.commit()
        for job in jobs:
            await db_session.refresh(job)
        
        # ç‚ºæ¯å€‹å·¥ä½œå‰µå»ºä¸åŒæ•¸é‡çš„è¨˜éŒ„å’ŒéŒ¯èª¤
        all_records = []
        all_errors = []
        
        for i, job in enumerate(jobs):
            # å·¥ä½œ1: 2ç­†è¨˜éŒ„, 1ç­†éŒ¯èª¤
            # å·¥ä½œ2: 3ç­†è¨˜éŒ„, 2ç­†éŒ¯èª¤
            # å·¥ä½œ3: 1ç­†è¨˜éŒ„, 0ç­†éŒ¯èª¤
            record_count = i + 2 if i < 2 else 1
            error_count = i + 1 if i < 2 else 0
            
            # å‰µå»ºè¨˜éŒ„ï¼ˆç¨ç«‹å­˜åœ¨ï¼Œä¸èˆ‡ç‰¹å®šå·¥ä½œé—œè¯ï¼‰
            for j in range(record_count):
                record_data = TestDataFactory.record_data(
                    lot_no=f"JOB{i+1}{j+1:02d}_01",
                    product_name=f"å·¥ä½œ{i+1}_ç”¢å“{j+1}",
                    quantity=100 + i * 10 + j,
                    production_date=date(2024, 1, (i+1) * 3 + j)
                )
                all_records.append(Record(**record_data))
            
            # å‰µå»ºéŒ¯èª¤ï¼ˆèˆ‡ç‰¹å®šå·¥ä½œé—œè¯ï¼‰
            for j in range(error_count):
                error_data = TestDataFactory.upload_error_data(
                    job_id=job.id,
                    row_index=j+1,
                    field="test_field",
                    error_code="ISOLATION_ERROR",
                    message=f"å·¥ä½œ{i+1}_éŒ¯èª¤{j+1}"
                )
                all_errors.append(UploadError(**error_data))
        
        db_session.add_all(all_records + all_errors)
        await db_session.commit()
        
        # é©—è­‰æ¯å€‹å·¥ä½œçš„è³‡æ–™éš”é›¢
        for i, job in enumerate(jobs):
            # æŸ¥è©¢è©²å·¥ä½œçš„éŒ¯èª¤
            job_errors_result = await db_session.execute(
                select(UploadError).where(UploadError.job_id == job.id)
            )
            job_errors = job_errors_result.scalars().all()
            
            # é©—è­‰æ•¸é‡
            expected_error_count = i + 1 if i < 2 else 0
            
            assert len(job_errors) == expected_error_count
            
            # é©—è­‰è³‡æ–™å±¬æ–¼æ­£ç¢ºçš„å·¥ä½œ
            assert all(error.job_id == job.id for error in job_errors)
            
            # é©—è­‰è³‡æ–™å…§å®¹éš”é›¢
            for error in job_errors:
                assert f"å·¥ä½œ{i+1}" in error.message
            
            print(f"âœ… å·¥ä½œ{i+1}: {len(job_errors)}ç­†éŒ¯èª¤ - éš”é›¢é©—è­‰é€šé")
        
        # é©—è­‰è¨˜éŒ„ç¸½æ•¸ï¼ˆæ‰€æœ‰è¨˜éŒ„éƒ½æ˜¯ç¨ç«‹çš„ï¼‰
        total_records_result = await db_session.execute(select(Record))
        total_records = total_records_result.scalars().all()
        expected_total_records = 2 + 3 + 1  # å„å·¥ä½œçš„è¨˜éŒ„æ•¸ç¸½å’Œ
        assert len(total_records) == expected_total_records
        print(f"âœ… ç¸½è¨˜éŒ„æ•¸é©—è­‰: {len(total_records)}ç­†è¨˜éŒ„ï¼ˆç¨ç«‹å­˜åœ¨ï¼‰")
    
    @pytest.mark.asyncio
    async def test_data_consistency_constraints(self, db_session, clean_db):
        """æ¸¬è©¦è³‡æ–™ä¸€è‡´æ€§ç´„æŸ"""
        
        # å‰µå»ºå·¥ä½œ
        job_data = TestDataFactory.upload_job_data()
        job = UploadJob(**job_data)
        db_session.add(job)
        await db_session.commit()
        await db_session.refresh(job)
        
        # æ¸¬è©¦è¨˜éŒ„çš„æ‰¹è™Ÿå”¯ä¸€æ€§ï¼ˆå…è¨±é‡è¤‡ï¼Œå› ç‚ºæ²’æœ‰å”¯ä¸€ç´„æŸï¼‰
        record1_data = TestDataFactory.record_data(
            lot_no="1234567_01",
            product_name="ç”¢å“1",
            quantity=100,
            production_date=date(2024, 1, 1)
        )
        record1 = Record(**record1_data)
        db_session.add(record1)
        await db_session.commit()
        
        # å˜—è©¦å‰µå»ºç›¸åŒæ‰¹è™Ÿçš„è¨˜éŒ„ï¼ˆæ‡‰è©²è¢«å…è¨±ï¼Œå› ç‚ºæ²’æœ‰å”¯ä¸€ç´„æŸï¼‰
        record2_data = TestDataFactory.record_data(
            lot_no="1234567_01",  # ç›¸åŒæ‰¹è™Ÿ
            product_name="ç”¢å“2",
            quantity=200,
            production_date=date(2024, 1, 2)
        )
        record2 = Record(**record2_data)
        db_session.add(record2)
        await db_session.commit()  # é€™æ‡‰è©²æˆåŠŸï¼Œå› ç‚ºæ²’æœ‰å”¯ä¸€ç´„æŸ
        
        # é©—è­‰å…©ç­†è¨˜éŒ„éƒ½å­˜åœ¨
        records_result = await db_session.execute(
            select(Record).where(Record.lot_no == "1234567_01")
        )
        records = records_result.scalars().all()
        assert len(records) == 2
        print("âœ… æ‰¹è™Ÿé‡è¤‡æ¸¬è©¦é€šéï¼ˆå…è¨±é‡è¤‡ï¼‰")
        
        # æ¸¬è©¦è³‡æ–™å‹æ…‹ä¸€è‡´æ€§
        test_records = []
        test_cases = [
            {
                "lot_no": "9999999_99",
                "product_name": "æ¸¬è©¦ç”¢å“A",
                "quantity": 0,  # é‚Šç•Œå€¼
                "production_date": date(1999, 12, 31)
            },
            {
                "lot_no": "0000001_01",
                "product_name": "æ¸¬è©¦ç”¢å“B",
                "quantity": 999999,  # å¤§æ•¸å€¼
                "production_date": date(2099, 12, 31)
            },
            {
                "lot_no": "1111111_11",
                "product_name": "æ¸¬è©¦ç”¢å“CåŒ…å«ä¸­æ–‡ğŸ‰ç‰¹æ®Šå­—å…ƒ!@#$%",
                "quantity": 12345,
                "production_date": date(2024, 2, 29)  # é–å¹´æ—¥æœŸ
            }
        ]
        
        for case in test_cases:
            record_data = TestDataFactory.record_data(**case)
            record = Record(**record_data)
            db_session.add(record)
            await db_session.commit()
            await db_session.refresh(record)
            test_records.append(record)
        
        # é©—è­‰è³‡æ–™å®Œæ•´æ€§
        for i, record in enumerate(test_records):
            case = test_cases[i]
            assert record.lot_no == case["lot_no"]
            assert record.product_name == case["product_name"]
            assert record.quantity == case["quantity"]
            assert record.production_date == case["production_date"]
        
        print("âœ… è³‡æ–™å‹æ…‹ä¸€è‡´æ€§æ¸¬è©¦é€šé")
        
        # æ¸¬è©¦æ™‚é–“æˆ³è¨˜ä¸€è‡´æ€§
        timestamps = []
        for i in range(3):
            record_data = TestDataFactory.record_data(
                lot_no=f"TIME{i:03d}_01",
                product_name=f"æ™‚é–“æ¸¬è©¦ç”¢å“{i}",
                quantity=100 + i,
                production_date=date(2024, 1, i+1)
            )
            record = Record(**record_data)
            db_session.add(record)
            await db_session.commit()
            await db_session.refresh(record)
            timestamps.append(record.created_at)
        
        # é©—è­‰æ™‚é–“æˆ³è¨˜éå¢ï¼ˆå®¹å¿æ¯«ç§’ç´šå·®ç•°ï¼‰
        for i in range(1, len(timestamps)):
            time_diff = (timestamps[i] - timestamps[i-1]).total_seconds()
            assert time_diff >= 0  # å¾Œå‰µå»ºçš„è¨˜éŒ„æ™‚é–“æ‡‰è©²ä¸æ—©æ–¼å‰ä¸€å€‹
            assert time_diff < 60   # ä½†å·®ç•°ä¸æ‡‰è¶…é1åˆ†é˜
        
        print("âœ… æ™‚é–“æˆ³è¨˜ä¸€è‡´æ€§æ¸¬è©¦é€šé")
        
        # æ¸¬è©¦ UploadError èˆ‡ UploadJob çš„é—œè¯ä¸€è‡´æ€§
        error_data = TestDataFactory.upload_error_data(
            job_id=job.id,
            row_index=100,
            field="consistency_test",
            error_code="CONSISTENCY_ERROR",
            message="ä¸€è‡´æ€§æ¸¬è©¦éŒ¯èª¤"
        )
        error = UploadError(**error_data)
        db_session.add(error)
        await db_session.commit()
        await db_session.refresh(error)
        
        assert error.job_id == job.id
        print("âœ… å¤–éµé—œè¯ä¸€è‡´æ€§æ¸¬è©¦é€šé")
        
        print(f"ğŸ“Š è³‡æ–™ä¸€è‡´æ€§æ¸¬è©¦å®Œæˆï¼šå…±æ¸¬è©¦ {len(test_records) + 3 + 2} ç­†è¨˜éŒ„")